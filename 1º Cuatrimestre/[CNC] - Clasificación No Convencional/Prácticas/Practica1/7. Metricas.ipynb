{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1: Clasificación multi-etiqueta\n",
    "\n",
    "## Ejercicio 7\n",
    "\n",
    "Describe brevemente las métricas que utilizaremos: \n",
    "- Accuracy\n",
    "- Hamming loss\n",
    "- Precision\n",
    "- Recall\n",
    "- F1_score\n",
    "\n",
    "La información que aporta cada una de estas métricas. Investiga su uso estudiando classification metrics y classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoque\n",
    "\n",
    "A continuación se describirán brevemente las métricas. Después se realizará una clasificación para poder comprobar el funcionamiento de las métricas de clasificación y `classification_report()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de Evaluación\n",
    "\n",
    "- **Accuracy:**\n",
    "   - Mide la proporción de predicciones correctas entre el total de predicciones.\n",
    "\n",
    "- **Hamming Loss:**\n",
    "   - Mide la fracción de etiquetas que son incorrectas en las predicciones.\n",
    "\n",
    "- **Precision:**\n",
    "   - Mide la proporción de verdaderos positivos entre el total de predicciones positivas.\n",
    "\n",
    "- **Recall:**\n",
    "   - Mide la proporción de verdaderos positivos entre el total de ejemplos positivos reales.\n",
    "\n",
    "- **F1 Score:**\n",
    "   - Combina la precisión y el recall en una única métrica que busca un equilibrio entre ambas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, _, _ = load_dataset('scene', 'train')\n",
    "X_test, y_test, _, _ = load_dataset('scene', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizará, como en apartados anteriores, el conjunto de datos scene y el clasificador NaiveBayes con el transformador LabelPowerset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5560200668896321\n",
      "Hamming Loss: 0.13447603121516166\n",
      "Precision: 0.6508299792050203\n",
      "Recall: 0.6335642802155504\n",
      "F1 Score: 0.6331829604236995\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.69       200\n",
      "           1       0.85      0.69      0.76       199\n",
      "           2       0.73      0.77      0.75       200\n",
      "           3       0.80      0.74      0.77       237\n",
      "           4       0.42      0.67      0.52       256\n",
      "           5       0.40      0.29      0.33       207\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1299\n",
      "   macro avg       0.66      0.63      0.64      1299\n",
      "weighted avg       0.65      0.63      0.63      1299\n",
      " samples avg       0.64      0.64      0.63      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un clasificador GaussianNB y un PowerLabelSet\n",
    "classifier = LabelPowerset(GaussianNB())\n",
    "\n",
    "# Entrenar el modelo\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las etiquetas en el conjunto de prueba\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar las métricas\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "hamming_loss = metrics.hamming_loss(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Hamming Loss: {hamming_loss}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "# Mostrar el informe de clasificación\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se ha realizado una clasificación para calcular las métricas obtenidas. Estas métricas se obtienen comparando las ground truth del test con las predicciones obtenidas en el modelo entrenado con train sobre los datos de test.\n",
    "\n",
    "Por un lado se han mostrado las métricas de forma individual, y por otro de ha utilizado `classification_report` que muestra directamente las métricas, diferenciando entre micro, macro, weightened y samples. Además, muestra las métricas para cada clase. Sin embargo, `classification_report` no muestra otras métricas como HammingLoss.\n",
    "\n",
    "Esto hace ver que, si se quiere obtener el valor medio de las métricas es más interesante utilizar directamente las métricas de clasificación o si se quiere saber una métrica que no es de las principales y más comúnmente utilizadas como Hamming Loss. Sin embargo, para un análisis más exhaustivo, es más interesante utilizar `classification_report`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
