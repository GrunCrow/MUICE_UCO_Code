{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0SY5QHlJHWPj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrunCrow/MUICE_UCO_Code/blob/main/1%C2%BA%20Cuatrimestre/%5BBDA%5D%20-%20Introducci%C3%B3n%20al%20Big%20Data%20An%C3%A1lisis/Pr%C3%A1cticas/Bloque%202/01_PySpark_dataframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Introducción a DataFrames en Spark (Python)</b>\n",
        "## <i>Big Data Analytics</i>\n",
        "\n",
        "Curso 2023/24\n",
        "\n",
        "Prof. *Dr. José Raúl Romero Salguero*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LUXZZJ93Ftqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver en este notebook los conceptos básicos de manejo de conjuntos de datos con *PySpark*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UrAEyNEG3w5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalación del entorno**\n",
        "## Instalación de Hadoop\n",
        "\n",
        "Instalamos la versión de Hadoop/Spark 3.2.4\n",
        "Se puede visitar el sitio de Apache Spark para descargar otra versión, siempre que se trate de una versión estable y con mantenimiento en Apache:\n",
        "\n",
        "https://spark.apache.org/downloads.html\n",
        "\n",
        "Se configuran posteriormente las variables de entorno `JAVA_HOME` y `SPARK_HOME`"
      ],
      "metadata": {
        "id": "oA8Fsl8E2YSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "iDM_FjfutRUk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La descarga de Hadoop puede tomar su tiempo, según la conexión disponible. En caso de que fallara, se puede descargar el archivo de Moodle y subirlo al espacio de ejecución. Se borra posteriormente de la máquina virtual el archivo `.tgz`"
      ],
      "metadata": {
        "id": "ygAAV5Hc3go6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gboXJnmYsHJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72fe4ca-38b1-4a5d-af18-21d59bea7f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-24 15:20:14--  https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 301183180 (287M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.4-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.2.4-bin-had 100%[===================>] 287.23M  17.7MB/s    in 17s     \n",
            "\n",
            "2024-01-24 15:20:32 (16.7 MB/s) - ‘spark-3.2.4-bin-hadoop3.2.tgz’ saved [301183180/301183180]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Descomentar las líneaa según la necesidad\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar -xf spark-3.2.4-bin-hadoop3.2.tgz\n",
        "#!rm spark-3.2.4-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación e iniciación de la sesión de Spark\n",
        "\n",
        "* Buscamos la librería `findspark` con `pip install`\n"
      ],
      "metadata": {
        "id": "gc1G4c_l4JzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "ouepVeD-Hg30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015ec3a6-e9a2-46ba-8b2d-793bd70fc141"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Con `SparkSession` inicializamos"
      ],
      "metadata": {
        "id": "wgNshvyOHhfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Spark_Dataframes\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "jXPu1jn_tYoC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "TmZVwuoCJ6pe",
        "outputId": "9fcc9fc1-2035-4c81-f266-738e34e53d60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7edcf07594e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c6ad6e40edc4:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark_Dataframes</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lectura del dataset**\n"
      ],
      "metadata": {
        "id": "8labSFrG42Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Para acceder al dataset, **solo si está en Google Drive**, debemos montar la unidad de Google Drive. Igualmente, es importante activarlo con el botón del menú de la izquierda.\n",
        "\n",
        "> Es posible que pida autenticación y autorización para acceder a Google Drive."
      ],
      "metadata": {
        "id": "7BoSRm7LxMcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Utilizar esta celda SOLO si el dataset lo tenemos en Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgYaaeBbub41",
        "outputId": "65a15b30-f599-454b-acb4-fe1b8c319c37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Leemos en spark el dataset (formato csv)"
      ],
      "metadata": {
        "id": "Womzd80qzd6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomentar la siguiente línea si el dataset se ha descargado de una ruta de Google Drive (actualizar ruta)\n",
        "# OJO: Comentar las dos últimas líneas !wget, spark.read si se descomenta esta.\n",
        "##ds = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/Material/Data/weblog.csv')\n",
        "\n",
        "# Si subimos el CSV directamente al espacio de la máquina virtual, lo cargamos directamente\n",
        "ds = spark.read.csv('weblog.csv')\n",
        "\n",
        "# Recuerda que los ficheros de este espacio de almacenamiento desaparecen cuando finaliza la ejecución de la máquina"
      ],
      "metadata": {
        "id": "BImWra-_sLM6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Realizamos varias operaciones básicas sobre la estructura, como:\n",
        "  * Imprimir el esquema con `printSchema`, incluyendo el nombre y tipo\n",
        "  * Mostrar las 5 primeras filas con `show`\n",
        "  * Contar el número de filas (tamaño del dataset) con `count`\n",
        "\n",
        "También podríamos obtener el nombre de la columna `i` con `.columns[i]`. De forma genérica, se le nombrará como \"_c\" y el índice de orden `_c0`, `_c1`, ...\n"
      ],
      "metadata": {
        "id": "LXvwQXK45E4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostramos el esquema del dataset\n",
        "ds.printSchema()\n",
        "# Mostramos las 5 primeras filas\n",
        "ds.show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt084YqExXd3",
        "outputId": "a2bb176e-6939-4f0c-be75-4d5a31ef430d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            "\n",
            "+----------+---------------------+-------------------------------------+-----+\n",
            "|_c0       |_c1                  |_c2                                  |_c3  |\n",
            "+----------+---------------------+-------------------------------------+-----+\n",
            "|IP        |Time                 |URL                                  |Staus|\n",
            "|10.128.2.1|[29/Nov/2017:06:58:55|GET /login.php HTTP/1.1              |200  |\n",
            "|10.128.2.1|[29/Nov/2017:06:59:02|POST /process.php HTTP/1.1           |302  |\n",
            "|10.128.2.1|[29/Nov/2017:06:59:03|GET /home.php HTTP/1.1               |200  |\n",
            "|10.131.2.1|[29/Nov/2017:06:59:04|GET /js/vendor/moment.min.js HTTP/1.1|200  |\n",
            "+----------+---------------------+-------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# El número total de filas - Tamaño del dataset\n",
        "ds.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h02AyUqX6BA-",
        "outputId": "83f17348-bb9d-44f1-f51b-c7facb7ac244"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16008"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección de filas y operaciones SQL sobre el dataframe\n",
        "Como si de SQL se tratara, podemos realizar un filtrado de las filas en base al valor de alguna de sus columnas utilizando métodos que nos devolverán un nuevo `DataFrame`.\n",
        "\n",
        "Por ejemplo, seleccionar aquellos accesos cuya respuesta HTTP es `200 OK`:"
      ],
      "metadata": {
        "id": "wSpoFe5s6IYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_200 = ds.filter(\"_c3 = 200\")\n",
        "ds_200.select(\"_c0\", \"_c2\").show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N82VsthQ1JAm",
        "outputId": "1459474b-a4e1-46b1-80ea-8dbff9058b69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------------------------------+\n",
            "|_c0       |_c2                                          |\n",
            "+----------+---------------------------------------------+\n",
            "|10.128.2.1|GET /login.php HTTP/1.1                      |\n",
            "|10.128.2.1|GET /home.php HTTP/1.1                       |\n",
            "|10.131.2.1|GET /js/vendor/moment.min.js HTTP/1.1        |\n",
            "|10.130.2.1|GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1|\n",
            "|10.130.2.1|GET /profile.php?user=bala HTTP/1.1          |\n",
            "|10.128.2.1|GET /js/jquery.min.js HTTP/1.1               |\n",
            "|10.131.2.1|GET /js/chart.min.js HTTP/1.1                |\n",
            "|10.131.2.1|GET /edit.php?name=bala HTTP/1.1             |\n",
            "|10.131.2.1|GET /login.php HTTP/1.1                      |\n",
            "|10.130.2.1|GET /login.php HTTP/1.1                      |\n",
            "+----------+---------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver arriba, `filter` equivale a la cláusula *WHERE* de un *SELECT*, mientras que `select` equivale a la proyección."
      ],
      "metadata": {
        "id": "yH34ncuu8ypV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos los códigos de respuesta en el rango 300\n",
        "ds_filtrado = ds.filter(\"_c3 >= 300 AND _c3 < 400\")\n",
        "print(\"Hay\",ds_filtrado.count(),\"accesos que devolvieron código de redirección:\")\n",
        "# Se muestran los *distintos* códigos de redirección que encontramos\n",
        "ds_filtrado.select(\"_c3\").distinct().show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o2gJDFQ8my0",
        "outputId": "b34a61b9-e8c3-4579-caa9-d095470d823e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hay 4156 accesos que devolvieron código de redirección:\n",
            "+---+\n",
            "|_c3|\n",
            "+---+\n",
            "|302|\n",
            "|304|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero, ¿cuántos accesos de cada tipo hay?"
      ],
      "metadata": {
        "id": "S-1NHXcTBRZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_filtrado = ds.filter(\"_c3 >= 300 AND _c3 < 400\")\n",
        "ds_filtrado.groupby(\"_c3\").count().show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiN3YzTD_X2f",
        "outputId": "6c23506a-a31c-4256-dca8-2f6eb64329b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|_c3|count|\n",
            "+---+-----+\n",
            "|302| 3498|\n",
            "|304|  658|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso de sentencias SQL\n",
        "\n",
        "Podemos utilizar directamente SQL sobre el DataFrame. Para ello, la sesión Spark (`spark`) debe estar inicializada.\n",
        "\n",
        "Se crea una tabla temporal en caché asociada al DataFrame sobre la que se ejecuta SQL.\n",
        "\n",
        "Ya podemos realizar operaciones y escribir el resultado en un DataFrame, si fuera necesario."
      ],
      "metadata": {
        "id": "ub0w42VQCDmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo**: Obtener aquellas instancias cuyo estado es ```200```."
      ],
      "metadata": {
        "id": "eE9pFXDnt52H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.createOrReplaceTempView(\"ds\")\n",
        "spark.sql(\"select * from ds where _c3 = 200\").show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BZ2j6PrBl0X",
        "outputId": "e3488018-1c34-462a-c7ae-a22d9978d8cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------+---------------------------------------------+---+\n",
            "|_c0       |_c1                  |_c2                                          |_c3|\n",
            "+----------+---------------------+---------------------------------------------+---+\n",
            "|10.128.2.1|[29/Nov/2017:06:58:55|GET /login.php HTTP/1.1                      |200|\n",
            "|10.128.2.1|[29/Nov/2017:06:59:03|GET /home.php HTTP/1.1                       |200|\n",
            "|10.131.2.1|[29/Nov/2017:06:59:04|GET /js/vendor/moment.min.js HTTP/1.1        |200|\n",
            "|10.130.2.1|[29/Nov/2017:06:59:06|GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1|200|\n",
            "|10.130.2.1|[29/Nov/2017:06:59:19|GET /profile.php?user=bala HTTP/1.1          |200|\n",
            "|10.128.2.1|[29/Nov/2017:06:59:19|GET /js/jquery.min.js HTTP/1.1               |200|\n",
            "|10.131.2.1|[29/Nov/2017:06:59:19|GET /js/chart.min.js HTTP/1.1                |200|\n",
            "|10.131.2.1|[29/Nov/2017:06:59:30|GET /edit.php?name=bala HTTP/1.1             |200|\n",
            "|10.131.2.1|[29/Nov/2017:06:59:37|GET /login.php HTTP/1.1                      |200|\n",
            "|10.130.2.1|[29/Nov/2017:07:00:19|GET /login.php HTTP/1.1                      |200|\n",
            "+----------+---------------------+---------------------------------------------+---+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo**: Obtener las IPs que han solicitado un recurso con extensión ```.php```y han devuelto un estado ```200```."
      ],
      "metadata": {
        "id": "8HGlYn7HuIeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_sql = spark.sql(\"SELECT DISTINCT(_c0) FROM ds WHERE _c2 LIKE '%.php%' AND _c3 = 200\")\n",
        "ds_sql.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TY8SCnBDFgS",
        "outputId": "c0531905-6cbe-4a7b-c8c8-3b06ce42f098"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|       _c0|\n",
            "+----------+\n",
            "|10.131.2.1|\n",
            "|10.128.2.1|\n",
            "|10.130.2.1|\n",
            "|10.131.0.1|\n",
            "|10.129.2.1|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Cuántas IPs se han obtenido?"
      ],
      "metadata": {
        "id": "RA2raYVBuYKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_sql.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlS7fCS1Fdjo",
        "outputId": "039df6c8-6b81-490c-e95c-bd25d9b5383f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finalización de la sesión de Spark"
      ],
      "metadata": {
        "id": "0SY5QHlJHWPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "PezbLTkmHRry"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Referencias</b>\n",
        "\n",
        "Información adicional sobre:\n",
        "\n",
        "* PySpark y el uso de Spark SQL en DataFrames: https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53\n",
        "\n",
        "* Disponible la *Spark SQL Guide*: https://spark.apache.org/docs/latest/sql-data-sources.html\n",
        "\n",
        "* Tutorial básico/guía de referencia de SQL: https://www.w3schools.com/sql/default.asp\n",
        "\n",
        "* Tutorial básico/guía de referencia de Python: https://www.w3schools.com/python/"
      ],
      "metadata": {
        "id": "9vv6TS7cC5MP"
      }
    }
  ]
}